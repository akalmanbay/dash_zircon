{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d20f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from termcolor import colored\n",
    "\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b74ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "c:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "c:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "c:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import treetaggerwrapper as tt\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    \"\"\"Sort a dict with highest score\"\"\"\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        top_k_keywords: int = 10,\n",
    "        top_n: int = 30,\n",
    "        stopwords: List[str] = None,\n",
    "        download_nltk=False,\n",
    "    ):\n",
    "        self.top_k_keywords = top_k_keywords\n",
    "        self.top_n = top_n\n",
    "\n",
    "        nltk.download(\"stopwords\")\n",
    "        self.stopwords = list(nltk_stopwords.words(\"english\"))\n",
    "        if stopwords is not None:\n",
    "            self.stopwords += stopwords\n",
    "\n",
    "        path = \"tree_tagger_lib\"\n",
    "        self.t_tagger = tt.TreeTagger(TAGLANG=\"en\", TAGDIR=path)\n",
    "\n",
    "    def _preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.apply(lambda x: str(x).lower())\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df.str.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\".\", \"\")))\n",
    "        df = df.str.replace(\"\\d+\", \"\")\n",
    "\n",
    "        # lemmatization\n",
    "        df = df.apply(lambda x: self.t_tagger.tag_text(x))\n",
    "        df = df.apply(lambda x: [t.split(\"\\t\")[-1] for t in x])\n",
    "        df = df.apply(lambda x: \" \".join(x))\n",
    "        return df.to_list()\n",
    "\n",
    "    def _extract_topn_from_vector(self, feature_names: List[str], sorted_items: Tuple[int, float]) -> Dict[str, float]:\n",
    "        \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "\n",
    "        # use only topn items from vector\n",
    "        sorted_items = sorted_items[: self.top_k_keywords]\n",
    "\n",
    "        score_vals = []\n",
    "        feature_vals = []\n",
    "\n",
    "        # word index and corresponding tf-idf score\n",
    "        for idx, score in sorted_items:\n",
    "            # keep track of feature name and its corresponding score\n",
    "            score_vals.append(round(score, 3))\n",
    "            feature_vals.append(feature_names[idx])\n",
    "\n",
    "        # create a tuples of feature, score\n",
    "        results = {}\n",
    "        for idx in range(len(feature_vals)):\n",
    "            results[feature_vals[idx]] = score_vals[idx]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _get_keywords(self, vectorizer, feature_names, doc):\n",
    "        \"\"\"Return top k keywords from a doc using TF-IDF method\"\"\"\n",
    "        # generate tf-idf for the given document\n",
    "        tf_idf_vector = vectorizer.transform([doc])\n",
    "\n",
    "        # sort the tf-idf vectors by descending order of scores\n",
    "        sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "        # extract only TOP_K_KEYWORDS\n",
    "        keywords = self._extract_topn_from_vector(feature_names, sorted_items)\n",
    "        return list(keywords.keys())\n",
    "\n",
    "    def get_top_keywords(self, df: pd.DataFrame):\n",
    "        corpora = self._preprocess(df)\n",
    "        vectorizer = TfidfVectorizer(stop_words=self.stopwords, smooth_idf=True, use_idf=True)\n",
    "        vectorizer.fit(corpora)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Get top_keywords from TFIDF for each document(review)\n",
    "        corpora_top_keywords = []\n",
    "        for doc in corpora:\n",
    "            d = {}\n",
    "            d[\"full_text\"] = doc\n",
    "            d[\"top_keywords\"] = self._get_keywords(vectorizer, feature_names, doc)\n",
    "            corpora_top_keywords.append(d)\n",
    "        corpora_top_keywords = pd.DataFrame(corpora_top_keywords)\n",
    "\n",
    "        word_frequency = defaultdict(int)\n",
    "\n",
    "        # Count weight for each word based on its position in top_keywords\n",
    "        for i_row in range(corpora_top_keywords.shape[0]):\n",
    "            words = corpora_top_keywords.iloc[i_row].top_keywords\n",
    "            for i, word in enumerate(words):\n",
    "                word_frequency[word] += 1 / (1 + i)\n",
    "\n",
    "        word_frequency = dict(sorted(word_frequency.items(), key=lambda item: item[1], reverse=True)[: self.top_n])\n",
    "        return word_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffbad6",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cfdd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('aws_reviews_sample.csv').review\n",
    "df = df.apply(lambda x: str(x).replace('\\n', '').replace('\\r', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba6ca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error unknown url type:\n",
      "[nltk_data]     https>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2637437",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nogay/nltk_data'\n    - 'c:\\\\Users\\\\nogay\\\\Desktop\\\\dash_zircon\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\nogay\\\\Desktop\\\\dash_zircon\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\nogay\\\\Desktop\\\\dash_zircon\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nogay\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nogay/nltk_data'\n    - 'c:\\\\Users\\\\nogay\\\\Desktop\\\\dash_zircon\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\nogay\\\\Desktop\\\\dash_zircon\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\nogay\\\\Desktop\\\\dash_zircon\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nogay\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\nlp model\\better pre-engineering-Copy1.ipynb Ячейка 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mrocks :\u001b[39m\u001b[39m\"\u001b[39m, lemmatizer\u001b[39m.\u001b[39;49mlemmatize(\u001b[39m\"\u001b[39;49m\u001b[39mrocks\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcorpora :\u001b[39m\u001b[39m\"\u001b[39m, lemmatizer\u001b[39m.\u001b[39mlemmatize(\u001b[39m\"\u001b[39m\u001b[39mcorpora\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# a denotes adjective in \"pos\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nogay/nltk_data'\n    - 'c:\\\\Users\\\\nogay\\\\Desktop\\\\dash_zircon\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\nogay\\\\Desktop\\\\dash_zircon\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\nogay\\\\Desktop\\\\dash_zircon\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nogay\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "  \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a024d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error unknown url type:\n",
      "[nltk_data]     https>\n"
     ]
    },
    {
     "ename": "TreeTaggerError",
     "evalue": "Bad TreeTagger directory: c:\\Users\\nogay\\Desktop\\dash_zircon\\nlp model\\tree_tagger_lib",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTreeTaggerError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\nlp model\\better pre-engineering-Copy1.ipynb Ячейка 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m extractor \u001b[39m=\u001b[39m Extractor()\n",
      "\u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\nlp model\\better pre-engineering-Copy1.ipynb Ячейка 5\u001b[0m in \u001b[0;36mExtractor.__init__\u001b[1;34m(self, top_k_keywords, top_n, stopwords, download_nltk)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X30sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstopwords \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m stopwords\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtree_tagger_lib\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X30sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_tagger \u001b[39m=\u001b[39m tt\u001b[39m.\u001b[39;49mTreeTagger(TAGLANG\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m, TAGDIR\u001b[39m=\u001b[39;49mpath)\n",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:1006\u001b[0m, in \u001b[0;36mTreeTagger.__init__\u001b[1;34m(self, **kargs)\u001b[0m\n\u001b[0;32m   1004\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mUsing treetaggerwrapper.py from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, osp\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39m))\n\u001b[0;32m   1005\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_language(kargs)\n\u001b[1;32m-> 1006\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_tagger(kargs)\n\u001b[0;32m   1007\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_preprocessor(kargs)\n\u001b[0;32m   1008\u001b[0m \u001b[39m# Note: TreeTagger process is started later, when really needed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:1053\u001b[0m, in \u001b[0;36mTreeTagger._set_tagger\u001b[1;34m(self, kargs)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagdir):\n\u001b[0;32m   1052\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mBad TreeTagger directory: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagdir)\n\u001b[1;32m-> 1053\u001b[0m     \u001b[39mraise\u001b[39;00m TreeTaggerError(\u001b[39m\"\u001b[39m\u001b[39mBad TreeTagger directory: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagdir)\n\u001b[0;32m   1054\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mtagdir=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagdir)\n\u001b[0;32m   1056\u001b[0m \u001b[39m# ----- Set subdirectories.\u001b[39;00m\n",
      "\u001b[1;31mTreeTaggerError\u001b[0m: Bad TreeTagger directory: c:\\Users\\nogay\\Desktop\\dash_zircon\\nlp model\\tree_tagger_lib"
     ]
    }
   ],
   "source": [
    "extractor = Extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b87a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7212867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "c:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "c:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "c:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "import treetaggerwrapper as tt\n",
    "def textclean(df):\n",
    "    df = df.apply(lambda x: x.lower())\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.str.translate(str.maketrans('', '', string.punctuation.replace('.','')))\n",
    "    df = df.str.replace('\\d+', '')\n",
    "    \n",
    "    #lemmatization\n",
    "    path = 'TreeTagger/tree-tagger-MacOSX-3.2.3'\n",
    "    t_tagger = tt.TreeTagger(TAGLANG ='en', TAGDIR =path)\n",
    "\n",
    "    df = df.apply(lambda x: t_tagger.tag_text(x))\n",
    "    df = df.apply(lambda x: [t.split('\\t')[-1] for t in x])\n",
    "    df = df.apply(lambda x: ' '.join(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6681bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nogay\\AppData\\Local\\Temp\\ipykernel_10596\\79344281.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df = df.str.replace('\\d+', '')\n"
     ]
    },
    {
     "ename": "TreeTaggerError",
     "evalue": "Bad TreeTagger directory: c:\\Users\\nogay\\Desktop\\dash_zircon\\nlp model\\TreeTagger\\tree-tagger-MacOSX-3.2.3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTreeTaggerError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\nlp model\\better pre-engineering-Copy1.ipynb Ячейка 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m textclean(df)\n",
      "\u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\nlp model\\better pre-engineering-Copy1.ipynb Ячейка 9\u001b[0m in \u001b[0;36mtextclean\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#lemmatization\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTreeTagger/tree-tagger-MacOSX-3.2.3\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m t_tagger \u001b[39m=\u001b[39m tt\u001b[39m.\u001b[39;49mTreeTagger(TAGLANG \u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m, TAGDIR \u001b[39m=\u001b[39;49mpath)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: t_tagger\u001b[39m.\u001b[39mtag_text(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nogay/Desktop/dash_zircon/nlp%20model/better%20pre-engineering-Copy1.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: [t\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m x])\n",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:1006\u001b[0m, in \u001b[0;36mTreeTagger.__init__\u001b[1;34m(self, **kargs)\u001b[0m\n\u001b[0;32m   1004\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mUsing treetaggerwrapper.py from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, osp\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39m))\n\u001b[0;32m   1005\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_language(kargs)\n\u001b[1;32m-> 1006\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_tagger(kargs)\n\u001b[0;32m   1007\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_preprocessor(kargs)\n\u001b[0;32m   1008\u001b[0m \u001b[39m# Note: TreeTagger process is started later, when really needed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nogay\\Desktop\\dash_zircon\\venv\\lib\\site-packages\\treetaggerwrapper.py:1053\u001b[0m, in \u001b[0;36mTreeTagger._set_tagger\u001b[1;34m(self, kargs)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagdir):\n\u001b[0;32m   1052\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mBad TreeTagger directory: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagdir)\n\u001b[1;32m-> 1053\u001b[0m     \u001b[39mraise\u001b[39;00m TreeTaggerError(\u001b[39m\"\u001b[39m\u001b[39mBad TreeTagger directory: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagdir)\n\u001b[0;32m   1054\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mtagdir=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagdir)\n\u001b[0;32m   1056\u001b[0m \u001b[39m# ----- Set subdirectories.\u001b[39;00m\n",
      "\u001b[1;31mTreeTaggerError\u001b[0m: Bad TreeTagger directory: c:\\Users\\nogay\\Desktop\\dash_zircon\\nlp model\\TreeTagger\\tree-tagger-MacOSX-3.2.3"
     ]
    }
   ],
   "source": [
    "df = textclean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf779558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(df, STOPWORDS, top_n = 30):\n",
    "    \n",
    "    STOPWORDS += list(stopwords.words('english'))\n",
    "    corpora = df.to_list() \n",
    "    vectorizer = TfidfVectorizer(stop_words=STOPWORDS, smooth_idf=True, use_idf=True)\n",
    "    vectorizer.fit(corpora)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get top_keywords from TFIDF for each document(review)\n",
    "    corpora_top_keywords = []\n",
    "    for doc in corpora:\n",
    "        d = {}\n",
    "        d['full_text'] = doc\n",
    "        d['top_keywords'] = get_keywords(vectorizer, feature_names, doc)\n",
    "        corpora_top_keywords.append(d)\n",
    "    corpora_top_keywords = pd.DataFrame(corpora_top_keywords)\n",
    "\n",
    "    from collections import defaultdict\n",
    "    word_frequency = defaultdict(int)\n",
    "    \n",
    "    # Count weight for each word based on its position in top_keywords\n",
    "    for i_row in range(corpora_top_keywords.shape[0]):\n",
    "        words = corpora_top_keywords.iloc[i_row].top_keywords\n",
    "        for i, word in enumerate(words):\n",
    "            word_frequency[word] += 1 / (1 + i)     \n",
    "\n",
    "    word_frequency = dict(sorted(word_frequency.items(), key=lambda item: item[1], reverse=True)[:top_n])\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3502d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud = WordCloud(background_color = 'white').generate_from_frequencies(dictionary)\n",
    "# plt.figure(figsize=(16,12))\n",
    "# plt.imshow(cloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135312b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4cd97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keyword = 'hold'\n",
    "x = keyword_wordcloud(df, original, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'basket'\n",
    "x = keyword_wordcloud(df, original, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eedc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'long'\n",
    "res = keyword_wordcloud(res.lemmatized, res.original, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'wish'\n",
    "res = keyword_wordcloud(res.lemmatized, res.original, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a675ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wordcloud(res.lemmatized)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "621ca5fe9d0682bde8e1dc65695fbc4839927dd1906687c7746640853824d14e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
